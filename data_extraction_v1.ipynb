{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sec_edgar_api import EdgarClients\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"0\":{\"cik_str\":320193,\"ticker\":\"AAPL\",\"title\":\"Apple Inc.\"},\"1\":{\"cik_str\":1045810,\"ticker\":\"NVDA\",\"title\":\"NVIDIA CORP\"},\"2\":{\"cik_str\":789019,\"ticker\":\"MSFT\",\"title\":\"MICROSOFT CORP\"},\"3\":{\"cik_str\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Sanjay Srinivasa (ssrin054@ucr.edu)\"\n",
    "}\n",
    "resp = requests.get(url, headers=headers)\n",
    "\n",
    "print(resp.status_code)  # Check if you get 200\n",
    "print(resp.text[:200])   # Print first 200 chars of the response\n",
    "\n",
    "data = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_500_companies():\n",
    "    \"\"\"\n",
    "    Returns a list of (ticker, CIK, company_name) tuples for the first 500\n",
    "    entries in the SEC's `company_tickers.json`.\n",
    "    Note: The order is just the order the SEC lists them in the JSON,\n",
    "    not sorted by market cap or anything else.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Sanjay Srinivasa (ssrin054@ucr.edu)\"\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    data = resp.json()  # structured as { \"0\": {...}, \"1\": {...}, ... }\n",
    "\n",
    "    top_500 = []\n",
    "    counter = 0\n",
    "    for _, info in data.items():\n",
    "        if counter >= 500:\n",
    "            break\n",
    "        \n",
    "        ticker = info[\"ticker\"].upper()\n",
    "        cik_str = str(info[\"cik_str\"]).zfill(10)  # zero-pad to 10 digits\n",
    "        company_name = info[\"title\"]\n",
    "        \n",
    "        top_500.append((ticker, cik_str, company_name))\n",
    "        counter += 1\n",
    "    \n",
    "    return top_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 500 companies.\n"
     ]
    }
   ],
   "source": [
    "top_500_companies = get_top_500_companies()\n",
    "print(f\"Retrieved {len(top_500_companies)} companies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to track progress and errors\n",
    "logging.basicConfig(\n",
    "    filename='edgar_download.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "\n",
    "# Initialize EdgarClient with a proper User-Agent\n",
    "# Replace with your actual information\n",
    "edgar = EdgarClient(user_agent=\"Sanjay Srinivasa (ssrin054@ucr.edu)\")\n",
    "\n",
    "# Define desired filing types\n",
    "DESIRED_FORMS = [\"10-K\", \"10-Q\"]\n",
    "\n",
    "# Define target years\n",
    "TARGET_YEARS = [\"2023\", \"2024\"]\n",
    "\n",
    "# Define the list of top 500 companies (example with a few entries)\n",
    "top_500_companies = [\n",
    "    ('AAPL', '0000320193', 'Apple Inc.'),\n",
    "    ('AMZN', '0001067983', 'Amazon.com, Inc.'),\n",
    "    ('MSFT', '0000789019', 'Microsoft Corporation'),\n",
    "    # ... add up to 500 companies\n",
    "]\n",
    "\n",
    "def extract_text_from_html(file_path):\n",
    "    \"\"\"\n",
    "    Extracts and cleans text from an HTML filing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Remove scripts and styles\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "        # Get text\n",
    "        text = soup.get_text(separator='\\n')\n",
    "        # Optional: Further cleaning can be done here\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def download_filing_html(cik, accession_number, primary_doc):\n",
    "    \"\"\"\n",
    "    Downloads the primary HTML document of a filing.\n",
    "    Returns the file path if successful, else None.\n",
    "    \"\"\"\n",
    "    # Remove dashes from the accession number\n",
    "    folder = accession_number.replace(\"-\", \"\")\n",
    "    filename = f\"{cik}_{accession_number}_{primary_doc}\"\n",
    "    \n",
    "    # Define a structured directory to save filings\n",
    "    directory = os.path.join(\"filings\", cik)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Already downloaded {filepath}, skipping.\")\n",
    "        logging.info(f\"Already downloaded {filepath}, skipping.\")\n",
    "        return filepath\n",
    "    \n",
    "    url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{folder}/{primary_doc}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Sanjay Srinivasa (ssrin054@ucr.edu)\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Saved {filepath}\")\n",
    "            logging.info(f\"Successfully downloaded {filepath}\")\n",
    "            return filepath\n",
    "        else:\n",
    "            print(f\"Error: Could not download from {url}, status code {response.status_code}\")\n",
    "            logging.error(f\"Failed to download {url}, status code {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception while downloading {url}: {e}\")\n",
    "        logging.error(f\"Exception while downloading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_filing(file_path, metadata):\n",
    "    \"\"\"\n",
    "    Processes the downloaded filing: extracts text and prints a preview for verification.\n",
    "    \"\"\"\n",
    "    text = extract_text_from_html(file_path)\n",
    "    if text:\n",
    "        words = text.split()\n",
    "        chunk_size = 1000  # You can adjust this as needed\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        print(f\"Extracted {len(chunks)} chunks from {file_path}\")\n",
    "        logging.info(f\"Extracted {len(chunks)} chunks from {file_path}\")\n",
    "        \n",
    "        # For verification, print the first 200 characters of the first chunk\n",
    "        if chunks:\n",
    "            print(f\"First chunk preview for {metadata['ticker']} {metadata['form']} ({metadata['date']}):\")\n",
    "            print(f\"{chunks[0][:200]}...\\n\")\n",
    "            logging.info(f\"First chunk preview for {file_path}:\\n{chunks[0][:200]}...\\n\")\n",
    "    else:\n",
    "        print(f\"No text extracted from {file_path}\")\n",
    "        logging.warning(f\"No text extracted from {file_path}\")\n",
    "\n",
    "def download_10k_10q_filings(cik, ticker, company_name, start_year=\"2023\", end_year=\"2024\", forms=DESIRED_FORMS):\n",
    "    \"\"\"\n",
    "    Downloads specified filings for a given CIK within the date range.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = edgar.get_submissions(cik=cik)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching submissions for CIK {cik}: {e}\")\n",
    "        logging.error(f\"Error fetching submissions for CIK {cik}: {e}\")\n",
    "        return\n",
    "    \n",
    "    filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    \n",
    "    form_list = filings.get(\"form\", [])\n",
    "    accession_numbers = filings.get(\"accessionNumber\", [])\n",
    "    primary_docs = filings.get(\"primaryDocument\", [])\n",
    "    filing_dates = filings.get(\"filingDate\", [])\n",
    "    \n",
    "    # Iterate over the filings\n",
    "    for i in range(len(form_list)):\n",
    "        form_type = form_list[i]\n",
    "        filing_date = filing_dates[i]\n",
    "        year = filing_date.split(\"-\")[0]  # Extract year from date string\n",
    "        \n",
    "        if form_type in forms and year in TARGET_YEARS:\n",
    "            acc_num = accession_numbers[i]\n",
    "            pdoc = primary_docs[i]\n",
    "            \n",
    "            file_path = download_filing_html(cik, acc_num, pdoc)\n",
    "            if file_path:\n",
    "                metadata = {\n",
    "                    \"ticker\": ticker,\n",
    "                    \"form\": form_type,\n",
    "                    \"date\": filing_date\n",
    "                }\n",
    "                process_filing(file_path, metadata)\n",
    "                \n",
    "                # Respect SEC's rate limiting\n",
    "                time.sleep(0.2)  # 200ms delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_filings_for_multiple_companies(companies, start_year=\"2023\", end_year=\"2024\", forms=DESIRED_FORMS):\n",
    "    \"\"\"\n",
    "    Downloads filings for multiple companies.\n",
    "    \"\"\"\n",
    "    for idx, (ticker, cik, company_name) in enumerate(companies, start=1):\n",
    "        print(f\"\\n[{idx}/{len(companies)}] Fetching filings for {ticker} ({company_name}) with CIK: {cik}\")\n",
    "        logging.info(f\"Fetching filings for {ticker} ({company_name}) with CIK: {cik}\")\n",
    "        download_10k_10q_filings(\n",
    "            cik=cik,\n",
    "            ticker=ticker,\n",
    "            company_name=company_name,\n",
    "            start_year=start_year,\n",
    "            end_year=end_year,\n",
    "            forms=forms\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/1] Fetching filings for AAPL (Apple Inc.) with CIK: 0000320193\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-24-000123_aapl-20240928.htm, skipping.\n",
      "Extracted 32 chunks from filings\\0000320193\\0000320193_0000320193-24-000123_aapl-20240928.htm\n",
      "First chunk preview for AAPL 10-K (2024-11-01):\n",
      "aapl-20240928 false 2024 FY 0000320193 P1Y P1Y P1Y P1Y http://fasb.org/us-gaap/2024#MarketableSecuritiesCurrent http://fasb.org/us-gaap/2024#MarketableSecuritiesNoncurrent http://fasb.org/us-gaap/2024...\n",
      "\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-24-000081_aapl-20240629.htm, skipping.\n",
      "Extracted 11 chunks from filings\\0000320193\\0000320193_0000320193-24-000081_aapl-20240629.htm\n",
      "First chunk preview for AAPL 10-Q (2024-08-02):\n",
      "aapl-20240629 false 2024 Q3 0000320193 --09-28 P1Y P1Y P1Y P1Y http://fasb.org/us-gaap/2023#MarketableSecuritiesCurrent http://fasb.org/us-gaap/2023#MarketableSecuritiesNoncurrent http://fasb.org/us-g...\n",
      "\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-24-000069_aapl-20240330.htm, skipping.\n",
      "Extracted 11 chunks from filings\\0000320193\\0000320193_0000320193-24-000069_aapl-20240330.htm\n",
      "First chunk preview for AAPL 10-Q (2024-05-03):\n",
      "aapl-20240330 false 2024 Q2 0000320193 --09-28 P1Y P1Y P1Y P1Y http://fasb.org/us-gaap/2023#MarketableSecuritiesCurrent http://fasb.org/us-gaap/2023#MarketableSecuritiesNoncurrent http://fasb.org/us-g...\n",
      "\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-24-000006_aapl-20231230.htm, skipping.\n",
      "Extracted 10 chunks from filings\\0000320193\\0000320193_0000320193-24-000006_aapl-20231230.htm\n",
      "First chunk preview for AAPL 10-Q (2024-02-02):\n",
      "aapl-20231230 false 2024 Q1 0000320193 --09-28 P1Y P1Y P1Y http://fasb.org/us-gaap/2023#MarketableSecuritiesCurrent http://fasb.org/us-gaap/2023#MarketableSecuritiesNoncurrent http://fasb.org/us-gaap/...\n",
      "\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-23-000106_aapl-20230930.htm, skipping.\n",
      "Extracted 32 chunks from filings\\0000320193\\0000320193_0000320193-23-000106_aapl-20230930.htm\n",
      "First chunk preview for AAPL 10-K (2023-11-03):\n",
      "aapl-20230930 false 2023 FY 0000320193 P1Y 67 P1Y 25 P1Y 7 1 http://fasb.org/us-gaap/2023#MarketableSecuritiesCurrent http://fasb.org/us-gaap/2023#MarketableSecuritiesNoncurrent http://fasb.org/us-gaa...\n",
      "\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-23-000077_aapl-20230701.htm, skipping.\n",
      "Extracted 11 chunks from filings\\0000320193\\0000320193_0000320193-23-000077_aapl-20230701.htm\n",
      "First chunk preview for AAPL 10-Q (2023-08-04):\n",
      "aapl-20230701 false 2023 Q3 0000320193 --09-30 P1Y 67 P1Y 26 P1Y 6 1 http://fasb.org/us-gaap/2023#MarketableSecuritiesCurrent http://fasb.org/us-gaap/2023#MarketableSecuritiesNoncurrent http://fasb.or...\n",
      "\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-23-000064_aapl-20230401.htm, skipping.\n",
      "Extracted 11 chunks from filings\\0000320193\\0000320193_0000320193-23-000064_aapl-20230401.htm\n",
      "First chunk preview for AAPL 10-Q (2023-05-05):\n",
      "aapl-20230401 false 2023 Q2 0000320193 --09-30 P1Y 65 P1Y 26 P1Y 7 2 0000320193 2022-09-25 2023-04-01 0000320193 us-gaap:CommonStockMember 2022-09-25 2023-04-01 0000320193 aapl:A1.375NotesDue2024Membe...\n",
      "\n",
      "Already downloaded filings\\0000320193\\0000320193_0000320193-23-000006_aapl-20221231.htm, skipping.\n",
      "Extracted 10 chunks from filings\\0000320193\\0000320193_0000320193-23-000006_aapl-20221231.htm\n",
      "First chunk preview for AAPL 10-Q (2023-02-03):\n",
      "aapl-20221231 false 2023 Q1 0000320193 --09-30 P1Y 63 P1Y 27 P1Y 8 2 0000320193 2022-09-25 2022-12-31 0000320193 us-gaap:CommonStockMember 2022-09-25 2022-12-31 0000320193 aapl:A1.375NotesDue2024Membe...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the download and processing for top 500 companies\n",
    "download_filings_for_multiple_companies(top_500_companies[:1], start_year=\"2023\", end_year=\"2024\", forms=DESIRED_FORMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (your_env_name)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
